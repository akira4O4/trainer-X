project_config:
  task: multitask
  work_dir: project/multi_task
  mlflow_uri: localhost
  mlflow_port: 5000
  mlflow_experiment_name: multi_task

train_config:
  topk: 2
  seed: 114514
  deterministic: true
  epoch: 0
  max_epoch: 100
  print_freq: 10
  workflow: [ [ train, 1 ], [ val, 1 ] ]
  amp: true
  accumulation_steps: 8 # if ==0 -> false

model_config:
  model_name: multi_task_shufflenetplus_v2_x1_0
  num_classes: 2
  mask_classes: 2
  pretrained: False
  model_path: ''
  gpu: 0 # -1==cpu
  strict: True
  map_location: cpu

classification_data_config:
  #---------------------------------------
  dataset:
    train:
      root: D:\llf\dataset\danyang\test\256\train
      wh: [ 256,256 ]
      letterbox: true
    val:
      root: D:\llf\dataset\danyang\test\256\train
      wh: [ 256,256 ]
      letterbox: true
  #---------------------------------------
  dataloader:
    train:
      batch_size: 32
      shuffle: False
      num_workers: 2
      pin_memory: True
      batch_sampler: BalancedBatchSampler #default:None
    val:
      batch_size: 2
      shuffle: false
      num_workers: 0
      pin_memory: True
  #---------------------------------------

segmentation_data_config:
  #---------------------------------------
  dataset:
    train:
      root: D:\llf\dataset\danyang\F_train\seg\train
      wh: [ 256,256 ]
    val:
      root: D:\llf\dataset\danyang\F_train\seg\train
      wh: [ 256,256 ] #need this if transform_resize is true
  #---------------------------------------
  dataloader:
    train:
      batch_size: 16
      shuffle: False
      num_workers: 2
      pin_memory: True
    val:
      batch_size: 2
      shuffle: false
      num_workers: 0
      pin_memory: True
  #---------------------------------------

optimizer_config:
  name: AdamW
  params: null
  lr: 0.001

lr_config:
  scheduler_step_in_batch: false
  #plan A
  name: LambdaLR
  optimizer: null
  lr_lambda: "lambda epoch: 1 / (epoch / 4 + 1)" #hook call
  #---------------------------------------
  #plan B
  # name: CosineAnnealingWarmRestarts
  # optimizer: null
  # T_0: 10
  # T_mult: 2

loss_config:
  loss_weights: [ 1,1,0.5 ]
  classification:
    CrossEntropyLoss: { }
  segmentation:
    PeriodLoss:
      top_k: 0.5
      weight: [ 1,1 ]

    DiceLoss:
      model: MULTICLASS_MODE
      log_loss: false
      from_logits: true
      smooth: 1
      ignore_index: null
      eps: 1.e-7

test_config:
  test_dir: D:\llf\dataset\danyang\F_train\seg\train
  weight: D:\llf\code\pytorch-lab\project\segmentation\runs\20240322_150546\weights\20240322_152042_Epoch15_epoch15_MIoU#0.9239.pth
  experiment_time: '20240322_150546'
  need_resize: False
  good_idx: 0
  sum_method: true
  need_segment: true
  cls_threshold: [ ]
  seg_threshold: [ ]

onnx_export_config:
  opset_version: 12
  model_path: ''
  output_name: 'multi_task.onnx'
  batch_size: 1
  input_h: 480
  input_w: 480
  channel: 3
  num_classes: 3
  mask_classes: 3
  input_names: [ 'images' ]
  output_names: [ 'output1','output2' ]
  is_simplify: true
  verbose: false
  #----------------------------
  is_dynamic: true
  dynamic_axes:
    images:
      0: 'batch'
    output1:
      0: 'batch'
    output2:
      0: 'batch'